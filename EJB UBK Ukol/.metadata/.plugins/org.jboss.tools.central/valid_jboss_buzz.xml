<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">New Features for Qute Templating Engine Support in Quarkus Tools for Visual Studio Code 1.13.0</title><link rel="alternate" href="https://quarkus.io/blog/vscode-quarkus-1.13.0-released/" /><author><name>Jessica He</name></author><id>https://quarkus.io/blog/vscode-quarkus-1.13.0-released/</id><updated>2023-04-19T00:00:00Z</updated><content type="html">Quarkus Tools for Visual Studio Code 1.13.0 has been released on the VS Code Marketplace and Open VSX. This release focuses on Qute Templating Engine Support by introducing support for more sections and improving template validation. New Features Notable Qute features included in Quarkus Tools for Visual Studio Code 1.13.0...</content><dc:creator>Jessica He</dc:creator></entry><entry><title type="html">Kogito 1.36.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/04/kogito-1-36-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/04/kogito-1-36-0-released.html</id><updated>2023-04-18T13:12:34Z</updated><content type="html">We are glad to announce that the Kogito 1.36.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Added support for CloudEvents to Serverless Workflow Knative custom function * Implement action condition on Serverless Workflow * Job Service embedded Quarkus extension * Added Workflow executor. This allows  embedded execution of workflows in a standard JVM.  * Added first draft of Workflow definition fluent API. This allows programmatically defining workflows.  * Added support for OnOverflow annotation. This allows users to control event publisher overflow policy through application properties.  * Fixed bug that prevents using JQ expressions in arrays.   For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.27.0 artifacts are available at the . A detailed changelog for 1.36.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Implementing C++20 semaphores</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/18/implementing-c20-semaphores" /><author><name>Thomas Rodgers</name></author><id>afeb7533-a077-44a1-a7d3-be636d8d1cc2</id><updated>2023-04-18T07:00:00Z</updated><published>2023-04-18T07:00:00Z</published><summary type="html">&lt;p&gt;C++20 introduces &lt;code&gt;counting_semaphore&lt;/code&gt; and &lt;code&gt;binary_semaphore&lt;/code&gt;, which support blocking &lt;code&gt;acquire()&lt;/code&gt; and non-blocking &lt;code&gt;try_acquire()&lt;/code&gt; as well as timed &lt;code&gt;try_acquire_for()&lt;/code&gt; and &lt;code&gt;try_acquire_until()&lt;/code&gt;. On platforms that support &lt;code&gt;__platform_wait()&lt;/code&gt;/&lt;code&gt;__platform_notify()&lt;/code&gt;, we select an implementation strategy based on &lt;code&gt;atomic&lt;/code&gt;; otherwise, we attempt to use POSIX semaphores.&lt;/p&gt; &lt;p&gt;If you missed the previous article, read it here: &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;Implementing C++20 atomic waiting in libstdc++&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Semaphores in C++&lt;/h2&gt; &lt;p&gt;Here's what the ISO C++ Standard has to say on the matter of semaphores:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;1 Class template counting_semaphore maintains an internal counter that is initialized when the semaphore is created. The counter is decremented when a thread acquires the semaphore, and is incremented when a thread releases the semaphore. If a thread tries to acquire the semaphore when the counter is zero, the thread will block until another thread increments the counter by releasing the semaphore.&lt;/p&gt; &lt;p&gt;2 least_max_value shall be non-negative; otherwise the program is ill-formed.&lt;/p&gt; &lt;p&gt;3 Concurrent invocations of the member functions of counting_semaphore, other than its destructor, do not introduce data races.&lt;/p&gt; &lt;h4&gt;void release(ptrdiff_t update = 1);&lt;/h4&gt; &lt;p&gt;8 Preconditions: update &gt;= 0 is true, and update &lt;= max() - counter is true.&lt;/p&gt; &lt;p&gt;9 Effects: Atomically execute counter += update. Then, unblocks any threads that are waiting for counter to be greater than zero.&lt;/p&gt; &lt;p&gt;10 Synchronization: Strongly happens before invocations of try_acquire that observe the result of the effects.&lt;/p&gt; &lt;p&gt;11 Throws: system_error when an exception is required (32.2.2).&lt;/p&gt; &lt;p&gt;12 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;h4&gt;bool try_acquire() noexcept;&lt;/h4&gt; &lt;p&gt;13 Effects: Attempts to atomically decrement counter if it is positive, without blocking. If counter is not decremented, there is no effect and try_acquire immediately returns. An implementation may fail to decrement counter even if it is positive. [Note 1 : This spurious failure is normally uncommon, but allows interesting implementations based on a simple compare and exchange (Clause 31). — end note] An implementation should ensure that try_acquire does not consistently return false in the absence of contending semaphore operations.&lt;/p&gt; &lt;p&gt;14 Returns: true if counter was decremented, otherwise false.&lt;/p&gt; &lt;h4&gt;void acquire();&lt;/h4&gt; &lt;p&gt;15 Effects: Repeatedly performs the following steps, in order:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;(15.1) — Evaluates try_acquire. If the result is true, returns.&lt;.li&gt;&lt;/li&gt; &lt;li&gt;(15.2) — Blocks on *this until counter is greater than zero.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;16 Throws: system_error when an exception is required (32.2.2).&lt;/p&gt; &lt;p&gt;17 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;h4&gt;template bool try_acquire_for(const chrono::duration&amp; rel_time); template bool try_acquire_until(const chrono::time_point&amp; abs_time);&lt;/h4&gt; &lt;p&gt;18 Effects: Repeatedly performs the following steps, in order:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;(18.1) — Evaluates try_acquire(). If the result is true, returns true.&lt;/li&gt; &lt;li&gt;(18.2) — Blocks on *this until counter is greater than zero or until the timeout expires. If it is unblocked by the timeout expiring, returns false. The timeout expires (32.2.4) when the current time is after abs_time (for try_acquire_until) or when at least rel_time has passed from the start of the function (for try_acquire_for).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;19 Throws: Timeout-related exceptions (32.2.4), or system_error when a non-timeout-related exception is required (32.2.2). 20 Error conditions: Any of the error conditions allowed for mutex types (32.5.4.2).&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;OK, so how to implement it?&lt;/h2&gt; &lt;p&gt;If we have POSIX semaphores available, the following type will be defined:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { using __clock_t = chrono::system_clock; #ifdef SEM_VALUE_MAX static constexpr ptrdiff_t _S_max = SEM_VALUE_MAX; #else static constexpr ptrdiff_t _S_max = _POSIX_SEM_VALUE_MAX; #endif explicit __platform_semaphore(ptrdiff_t __count) noexcept { sem_init(&amp;_M_semaphore, 0, __count); } ~__platform_semaphore() { sem_destroy(&amp;_M_semaphore); } // ... }; &lt;/pre&gt; &lt;p&gt;And implements &lt;code&gt;acquire&lt;/code&gt;/​​​​​​&lt;code&gt;try_acquire&lt;/code&gt;/&lt;code&gt;release&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { // ... void _M_acquire() noexcept { for (;;) { auto __err = sem_wait(&amp;_M_semaphore); if (__err &amp;&amp; (errno == EINTR)) continue; else if (__err) std::terminate(); else break; } } _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire() noexcept { for (;;) { auto __err = sem_trywait(&amp;_M_semaphore); if (__err &amp;&amp; (errno == EINTR)) continue; else if (__err &amp;&amp; (errno == EAGAIN)) return false; else if (__err) std::terminate(); else break; } return true; } _GLIBCXX_ALWAYS_INLINE void _M_release(std::ptrdiff_t __update) noexcept { for(; __update != 0; --__update) { auto __err = sem_post(&amp;_M_semaphore); if (__err) std::terminate(); } } // ... }; &lt;/pre&gt; &lt;p&gt;For the timed wait operations, we have to worry about what the "basis" clock is and how to convert the user-supplied clock. For POSIX semaphores, this is handled as follows:&lt;/p&gt; &lt;pre&gt; struct __platform_semaphore { using __clock_t = chrono::system_clock; // ... bool _M_try_acquire_until_impl(const chrono::time_point&lt;__clock_t&gt;&amp; __atime) noexcept { auto __s = chrono::time_point_cast(__atime); auto __ns = chrono::duration_cast(__atime - __s); struct timespec __ts = { static_cast(__s.time_since_epoch().count()), static_cast(__ns.count()) }; for (;;) { if (auto __err = sem_timedwait(&amp;_M_semaphore, &amp;__ts)) { if (errno == EINTR) continue; else if (errno == ETIMEDOUT || errno == EINVAL) return false; else std::terminate(); } else break; } return true; } template bool _M_try_acquire_until(const chrono::time_point&lt;_Clock, _Duration&gt;&amp; __atime) noexcept { if constexpr (std::is_same_v&lt;__clock_t, _Clock&gt;) { return _M_try_acquire_until_impl(__atime); } else { const typename _Clock::time_point __c_entry = _Clock::now(); const auto __s_entry = __clock_t::now(); const auto __delta = __atime - __c_entry; const auto __s_atime = __s_entry + __delta; if (_M_try_acquire_until_impl(__s_atime)) return true; // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. return (_Clock::now() &lt; __atime); } } }; &lt;/pre&gt; &lt;p&gt;If we have support for &lt;code&gt;atomic::wait&lt;/code&gt;/&lt;code&gt;notify_one&lt;/code&gt;/&lt;code&gt;all&lt;/code&gt; then the following type will be defined:&lt;/p&gt; &lt;pre&gt; struct __atomic_semaphore { static constexpr ptrdiff_t _S_max = __gnu_cxx::__int_traits::__max; explicit __atomic_semaphore(__detail::__platform_wait_t __count) noexcept : _M_counter(__count) { __glibcxx_assert(__count &gt;= 0 &amp;&amp; __count &lt;= _S_max); } static _GLIBCXX_ALWAYS_INLINE bool _S_do_try_acquire(__detail::__platform_wait_t* __counter) noexcept { auto __old = __atomic_impl::load(__counter, memory_order::acquire); if (__old == 0) return false; return __atomic_impl::compare_exchange_strong(__counter, __old, __old - 1, memory_order::acquire, memory_order::relaxed); } _GLIBCXX_ALWAYS_INLINE void _M_acquire() noexcept { auto const __pred = [this] {return _S_do_try_acquire(&amp;this-&gt;_M_counter);}; std::__atomic_wait_address(&amp;_M_counter, __pred); } bool _M_try_acquire() noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return std::__detail::__atomic_spin(__pred); } // ... }; &lt;/pre&gt; &lt;p&gt;This expects an implementation of &lt;code&gt;__atomic_wait_address&lt;/code&gt; that can accept a predicate; however, we have only defined &lt;code&gt;__atomic_wait_address_v&lt;/code&gt; to this point (see &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;part 1&lt;/a&gt;).&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __waiter { // ... template&lt;typename _Pred&gt; void _M_do_wait(_Pred __pred) noexcept { do { __platform_wait_t __val; if (__base_type::_M_do_spin(__pred, __val)) return; __base_type::_M_w._M_do_wait(__base_type::_M_addr, __val); } while (!__pred()); } }; template&lt;typename _Tp, typename _Pred&gt; void __atomic_wait_address(const _Tp* __addr, _Pred __pred) noexcept { __detail::__enters_wait __w(__addr); __w._M_do_wait(__pred); } &lt;/pre&gt; &lt;p&gt;There is a problem with this formulation, in that &lt;code&gt;__enters_wait&lt;/code&gt; will atomically increment and decrement a waiter count, which &lt;code&gt;__atomic_semaphore&lt;/code&gt; does not need, as it inherently tracks the presence of waiters. In the previous article, we introduced the notion of &lt;code&gt;__enters_wait&lt;/code&gt; and &lt;code&gt;__bare_wait&lt;/code&gt; types to be used by waiters and notifiers, respectively. We extend that notion also to support "bare" waiters.&lt;/p&gt; &lt;pre&gt; // This call is to be used by atomic types which track contention externally template void __atomic_wait_address_bare(const __detail::__platform_wait_t* __addr, _Pred __pred) noexcept { #ifdef _GLIBCXX_HAVE_PLATFORM_WAIT do { __detail::__platform_wait_t __val; if (__detail::__bare_wait::_S_do_spin(__addr, __pred, __val)) return; __detail::__platform_wait(__addr, __val); } while (!__pred()); #else // !_GLIBCXX_HAVE_PLATFORM_WAIT __detail::__bare_wait __w(__addr); __w._M_do_wait(__pred); #endif } &lt;/pre&gt; &lt;p&gt;We also need to introduce a corresponding "bare" &lt;code&gt;notify&lt;/code&gt;, which skips checks for waiters:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool { // ... void _M_notify(const __platform_wait_t* __addr, bool __all, bool __bare) noexcept { if (!(__bare || _M_waiting())) return; #ifdef _GLIBCXX_HAVE_PLATFORM_WAIT __platform_notify(__addr, __all); #else if (__all) _M_cv.notify_all(); else _M_cv.notify_one(); #endif } }; &lt;/pre&gt; &lt;h3&gt;Supporting timed waits on __atomic_semaphore&lt;/h3&gt; &lt;p&gt;The C++ Standard has extensive support for durations and time points and specifies waiting operations in terms of those types:&lt;/p&gt; &lt;pre&gt; template&lt;class Rep, class Period&gt; bool try_acquire_for(const chrono::duration&lt;Rep, Period&gt;&amp; rel_time); template&lt;class Clock, class Duration&gt; bool try_acquire_until(const chrono::time_point&lt;Clock, Duration&gt;&amp; abs_time); &lt;/pre&gt; &lt;p&gt;However, &lt;code&gt;&lt;chrono&gt;&lt;/code&gt; is not supported in the freestanding (non-hosted) subset of the standard library, but &lt;code&gt;&lt;atomic&gt;&lt;/code&gt; is. This means we can't inject a dependency on the header or in its underlying &lt;code&gt;bits/atomic_base.h&lt;/code&gt; header. For this reason, all timed waiting functionality is split to &lt;code&gt;bits/atomic_timed_wait.h&lt;/code&gt;. We preferentially use &lt;code&gt;std::chrono::steady_clock&lt;/code&gt; as our basis clock on platforms that have futexes or &lt;code&gt;pthread_cond_clockwait&lt;/code&gt;. Otherwise, we fall back to the &lt;code&gt;system_clock&lt;/code&gt; and convert all user-supplied time points to this clock:&lt;/p&gt; &lt;pre&gt; #ifdef _GLIBCXX_HAVE_LINUX_FUTEX || _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT using __wait_clock_t = chrono::steady_clock; #else using __wait_clock_t = chrono::system_clock; #endif template&lt;typename _Clock, typename _Dur&gt; __wait_clock_t::time_point __to_wait_clock(const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { const typename _Clock::time_point __c_entry = _Clock::now(); const __wait_clock_t::time_point __w_entry = __wait_clock_t::now(); const auto __delta = __atime - __c_entry; using __w_dur = typename __wait_clock_t::duration; return __w_entry + chrono::ceil&lt;__w_dur&gt;(__delta); } template&lt;typename _Dur&gt; __wait_clock_t::time_point __to_wait_clock(const chrono::time_point&lt;__wait_clock_t, _Dur&gt;&amp; __atime) noexcept { using __w_dur = typename __wait_clock_t::duration; return chrono::ceil&lt;__w_dur&gt;(__atime); } &lt;/pre&gt; &lt;p&gt;If a platform supports an efficient &lt;code&gt;__platform_wait()&lt;/code&gt;, it is also expected to provide an efficient timed wait version of the same called &lt;code&gt;__platform_wait_until()&lt;/code&gt;. For Linux Futexes, we implement this as:&lt;/p&gt; &lt;pre&gt; // returns true if wait ended before timeout template&lt;typename _Dur&gt; bool __platform_wait_until_impl(const __platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;__wait_clock_t, _Dur&gt;&amp; __atime) noexcept { auto __s = chrono::time_point_cast&lt;chrono::seconds&gt;(__atime); auto __ns = chrono::duration_cast&lt;chrono::nanoseconds&gt;(__atime - __s); struct timespec __rt = { static_cast&lt;std::time_t&gt;(__s.time_since_epoch().count()), static_cast&lt;long&gt;(__ns.count()) }; auto __e = syscall (SYS_futex, __addr, static_cast&lt;int&gt;(__futex_wait_flags::__wait_bitset_private), __old, &amp;__rt, nullptr, static_cast&lt;int&gt;(__futex_wait_flags:: __bitset_match_any)); if (__e) { if ((errno != ETIMEDOUT) &amp;&amp; (errno != EINTR) &amp;&amp; (errno != EAGAIN)) __throw_system_error(errno); return true; } return false; } // returns true if wait ended before timeout template&lt;typename _Clock, typename _Dur&gt; bool __platform_wait_until(const __platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { if constexpr (is_same_v&lt;__wait_clock_t, _Clock&gt;) { return __platform_wait_until_impl(__addr, __old, __atime); } else { if (!__platform_wait_until_impl(__addr, __old, __to_wait_clock(__atime))) { // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. if (_Clock::now() &lt; __atime) return true; } return false; } } &lt;/pre&gt; &lt;p&gt;As with &lt;code&gt;__platform_wait()&lt;/code&gt;, for platforms that do not provide &lt;code&gt;__platform_wait_until()&lt;/code&gt;, we fall back to using a mutex and condition variable and defining the following support functions:&lt;/p&gt; &lt;pre&gt; // Returns true if wait ended before timeout. // _Clock must be either steady_clock or system_clock. template&lt;typename _Clock, typename _Dur&gt; bool __cond_wait_until_impl(__condvar&amp; __cv, mutex&amp; __mx, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { static_assert(std::__is_one_of&lt;_Clock, chrono::steady_clock, chrono::system_clock&gt;::value); auto __s = chrono::time_point_cast&lt;chrono::seconds&gt;(__atime); auto __ns = chrono::duration_cast&lt;chrono::nanoseconds&gt;(__atime - __s); __gthread_time_t __ts = { static_cast&lt;std::time_t&gt;(__s.time_since_epoch().count()), static_cast&lt;long&lt;(__ns.count()) }; // if we have a pthreads implementation that supports CLOCK_MONOTONIC // and the caller's clock is steady_clock, use that #ifdef _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT if constexpr (is_same_v&lt;chrono::steady_clock, _Clock&gt;) __cv.wait_until(__mx, CLOCK_MONOTONIC, __ts); else #endif __cv.wait_until(__mx, __ts); return _Clock::now() &amp; __atime; } // returns true if wait ended before timeout template&lt;typename _Clock, typename _Dur&gt; bool __cond_wait_until(__condvar&amp; __cv, mutex&amp; __mx, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { #ifdef _GLIBCXX_USE_PTHREAD_COND_CLOCKWAIT if constexpr (is_same_v&lt;_Clock, chrono::steady_clock&gt;) return __detail::__cond_wait_until_impl(__cv, __mx, __atime); else #endif if constexpr (is_same_v&lt;_Clock, chrono::system_clock&gt;) return __detail::__cond_wait_until_impl(__cv, __mx, __atime); else { if (__cond_wait_until_impl(__cv, __mx, __to_wait_clock(__atime))) { // We got a timeout when measured against __clock_t but // we need to check against the caller-supplied clock // to tell whether we should return a timeout. if (_Clock::now() &lt; __atime) return true; } return false; } } &lt;/pre&gt; &lt;p&gt;The formulation of &lt;code&gt;__waiter_pool&lt;/code&gt; from &lt;a href="https://developers.redhat.com/articles/2022/12/06/implementing-c20-atomic-waiting-libstdc"&gt;part 1&lt;/a&gt; assumes that we are in the market for indefinitely blocking waits. However, in this case, we only care about timed waiting. The actual implementation in &lt;code&gt;bits/atomic_wait.h&lt;/code&gt; is:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool_base { // ... alignas(_S_align) __platform_wait_t _M_wait = 0; #ifndef _GLIBCXX_HAVE_PLATFORM_WAIT mutex _M_mtx; #endif alignas(_S_align) __platform_wait_t _M_ver = 0; #ifndef _GLIBCXX_HAVE_PLATFORM_WAIT __condvar _M_cv; #endif void _M_enter_wait() noexcept; void _M_leave_wait() noexcept; bool _M_waiting() const noexcept; void _M_notify(const __platform_wait_t* __addr, bool __all, bool __bare) noexcept; static __waiter_pool_base&amp; _S_for(const void* __addr) noexcept; }; &lt;/pre&gt; &lt;p&gt;For indefinite atomic waits, the derived type &lt;code&gt;__waiter_pool&lt;/code&gt; is used:&lt;/p&gt; &lt;pre&gt; struct __waiter_pool : __waiter_pool_base { void _M_do_wait(const __platform_wait_t* __addr, __platform_wait_t __old) noexcept; }; &lt;/pre&gt; &lt;p&gt;For timed atomic waits, the derived type &lt;code&gt;__timed_waiter_pool&lt;/code&gt; is used:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter_pool : __waiter_pool_base { // returns true if wait ended before timeout template bool _M_do_wait_until(__platform_wait_t* __addr, __platform_wait_t __old, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { #ifdef _GLIBCXX_HAVE_PLATFORM_TIMED_WAIT return __platform_wait_until(__addr, __old, __atime); #else __platform_wait_t __val; __atomic_load(__addr, &amp;__val, __ATOMIC_RELAXED); if (__val == __old) { lock_guard __l(_M_mtx); return __cond_wait_until(_M_cv, _M_mtx, __atime); } #endif // _GLIBCXX_HAVE_PLATFORM_TIMED_WAIT } }; &lt;/pre&gt; &lt;p&gt;Similarly, &lt;code&gt;__waiter&lt;/code&gt; is split into a &lt;code&gt;__waiter_base&lt;/code&gt; type:&lt;/p&gt; &lt;pre&gt; template&lt;typename _Tp&gt; struct __waiter_base { using __waiter_type = _Tp; __waiter_type&amp; _M_w; __platform_wait_t* _M_addr; template static __platform_wait_t* _S_wait_addr(const _Up* __a, __platform_wait_t* __b); static __waiter_type&amp; _S_for(const void* __addr) noexcept { static_assert(sizeof(__waiter_type) == sizeof(__waiter_pool_base)); auto&amp; res = __waiter_pool_base::_S_for(__addr); return reinterpret_cast&lt;__waiter_type&amp;&gt;(res); } template&lt;typename _Up&gt; explicit __waiter_base(const _Up* __addr) noexcept : _M_w(_S_for(__addr)) , _M_addr(_S_wait_addr(__addr, &amp;_M_w._M_ver)) { } void _M_notify(bool __all, bool __bare = false); template&lt;typename _Up, typename _ValFn, typename _Spin = __default_spin_policy&gt; static bool _S_do_spin_v(__platform_wait_t* __addr, const _Up&amp; __old, _ValFn __vfn, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Up, typename _ValFn, typename _Spin = __default_spin_policy&gt; bool _M_do_spin_v(const _Up&amp; __old, _ValFn __vfn, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Pred, typename _Spin = __default_spin_policy&gt; static bool _S_do_spin(const __platform_wait_t* __addr, _Pred __pred, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); template&lt;typename _Pred, typename _Spin = __default_spin_policy&gt; bool _M_do_spin(_Pred __pred, __platform_wait_t&amp; __val, _Spin __spin = _Spin{ }); }; &lt;/pre&gt; &lt;p&gt;From which &lt;code&gt;__waiter&lt;/code&gt; is derived:&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __waiter : __waiter_base&lt;__waiter_pool&gt; { using __base_type = __waiter_base&lt;__waiter_pool&gt;; template&lt;typename _Tp&gt; __waiter(const _Tp* __addr) noexcept; // ... template&lt;typename _Tp, typename _ValFn&gt; void _M_do_wait_v(_Tp __old, _ValFn __vfn); template&lt;typename _Pred&gt; void _M_do_wait(_Pred __pred) noexcept; }; using __enters_wait = __waiter&lt;std::true_type&gt;; using __bare_wait = __waiter&lt;std::false_type&gt;; &lt;/pre&gt; &lt;p&gt;&lt;code&gt;__timed_waiter&lt;/code&gt; is similarly derived:&lt;/p&gt; &lt;pre&gt; template&lt;typename _EntersWait&gt; struct __timed_waiter : __waiter_base&lt;__timed_waiter_pool&gt; { using __base_type = __waiter_base&lt;__timed_waiter_pool&gt;; template&lt;typename _Tp&gt; __timed_waiter(const _Tp* __addr) noexcept; // ... // returns true if wait ended before timeout template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool _M_do_wait_until_v(_Tp __old, _ValFn __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; // returns true if wait ended before timeout template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, __platform_wait_t __val, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; // returns true if wait ended before timeout template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept; template&lt;typename _Tp, typename _ValFn, typename _Rep, typename _Period&gt; bool _M_do_wait_for_v(_Tp __old, _ValFn __vfn, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept; template&lt;typename _Pred, typename _Rep, typename _Period&gt; bool _M_do_wait_for(_Pred __pred, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept; }; using __enters_timed_wait = __timed_waiter&lt;std::true_type&gt;; using __bare_timed_wait = __timed_waiter&lt;std::false_type&gt;; &lt;/pre&gt; &lt;p&gt;As with &lt;code&gt;__waiter&lt;/code&gt;, there are top-level &lt;code&gt;__atomic_wait_address_until&lt;/code&gt;/&lt;code&gt;for&lt;/code&gt; wrappers that &lt;code&gt;__atomic_semaphore&lt;/code&gt; calls into, all of which follow the general form of:&lt;/p&gt; &lt;pre&gt; // returns true if wait ended before timeout template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool __atomic_wait_address_until_v(const _Tp* __addr, _Tp&amp;&amp; __old, _ValFn&amp;&amp; __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __detail::__enters_timed_wait __w{__addr}; return __w._M_do_wait_until_v(__old, __vfn, __atime); } template&lt;typename _Tp, typename _Pred, typename _Clock, typename _Dur&gt; bool __atomic_wait_address_until(const _Tp* __addr, _Pred __pred, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __detail::__enters_timed_wait __w{__addr}; return __w._M_do_wait_until(__pred, __atime); } &lt;/pre&gt; &lt;p&gt;With versions for "bare" wait and the &lt;code&gt;_for_v/_&lt;/code&gt;for variants to wait for a supplied duration. The actual waiting entry points on &lt;code&gt;__timed_waiter&lt;/code&gt; are implemented as follows:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter { // ... template&lt;typename _Tp, typename _ValFn, typename _Clock, typename _Dur&gt; bool _M_do_wait_until_v(_Tp __old, _ValFn __vfn, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { __platform_wait_t __val; if (_M_do_spin(__old, std::move(__vfn), __val, __timed_backoff_spin_policy(__atime))) return true; return __base_type::_M_w._M_do_wait_until(__base_type::_M_addr, __val, __atime); } template&lt;typename _Pred, typename _Clock, typename _Dur&gt; bool _M_do_wait_until(_Pred __pred, __platform_wait_t __val, const chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) noexcept { for (auto __now = _Clock::now(); __now &lt; __atime; __now = _Clock::now()) { if (__base_type::_M_do_spin(__pred, __val, __timed_backoff_spin_policy(__atime, __now))) return true; if (__base_type::_M_w._M_do_wait_until(__base_type::_M_addr, __val, __atime) &amp;&amp; __pred()) return true; } return false; } }; &lt;/pre&gt; &lt;p&gt;Aside from the usual differences between the &lt;code&gt;_v&lt;/code&gt; and predicate forms of wait, the timed waits introduce a custom spin policy:&lt;/p&gt; &lt;pre&gt; struct __timed_backoff_spin_policy { __wait_clock_t::time_point _M_deadline; __wait_clock_t::time_point _M_t0; template&lt;typename _Clock, typename _Dur&gt; __timed_backoff_spin_policy(chrono::time_point&lt;_Clock, _Dur&gt; __deadline = _Clock::time_point::max(), chrono::time_point&lt;_Clock, _Dur&gt; __t0 = _Clock::now()) noexcept : _M_deadline(__to_wait_clock(__deadline)) , _M_t0(__to_wait_clock(__t0)) { } bool operator()() const noexcept { using namespace literals::chrono_literals; auto __now = __wait_clock_t::now(); if (_M_deadline &lt;= __now) return false; auto __elapsed = __now - _M_t0; if (__elapsed &gt; 128ms) { this_thread::sleep_for(64ms); } else if (__elapsed &gt; 64us) { this_thread::sleep_for(__elapsed / 2); } else if (__elapsed &gt; 4us) { __thread_yield(); } else return false; return true; } }; &lt;/pre&gt; &lt;p&gt;After the usual spin completes unsatisfied, this will begin sleeping the current thread as long as the deadline hasn't been reached. The relative &lt;code&gt;wait_for&lt;/code&gt; variants are implemented in terms of the absolute &lt;code&gt;wait_until&lt;/code&gt; members:&lt;/p&gt; &lt;pre&gt; struct __timed_waiter { // ... template&lt;typename _Tp, typename _ValFn, typename _Rep, typename _Period&gt; bool _M_do_wait_for_v(_Tp __old, _ValFn __vfn, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { __platform_wait_t __val; if (_M_do_spin_v(__old, std::move(__vfn), __val)) return true; if (!__rtime.count()) return false; // no rtime supplied, and spin did not acquire auto __reltime = chrono::ceil&lt;__wait_clock_t::duration&gt;(__rtime); return __base_type::_M_w._M_do_wait_until(_base_type::_M_addr, __val, chrono::steady_clock::now() + __reltime); } template&lt;typename _Pred, typename _Rep, typename _Period&gt; bool _M_do_wait_for(_Pred __pred, const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { __platform_wait_t __val; if (__base_type::_M_do_spin(__pred, __val)) return true; if (!__rtime.count()) return false; // no rtime supplied, and spin did not acquire auto __reltime = chrono::ceil&lt;__wait_clock_t::duration&gt;(__rtime); return _M_do_wait_until(__pred, __val, chrono::steady_clock::now() + __reltime); } }; &lt;/pre&gt; &lt;h3&gt;The rest of __atomic_semaphore&lt;/h3&gt; &lt;p&gt;With support for atomic timed waits in place, we can define the remaining members of &lt;code&gt;__atomic_semaphore&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; struct __atomic_semaphore { // ... template&lt;typename _Clock, typename _Duration&gt; _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire_until(const chrono::time_point&lt;_Clock, _Duration&gt;&amp; __atime) noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return __atomic_wait_address_until_bare(&amp;_M_counter, __pred, __atime); } template&lt;typename _Rep, typename _Period&gt; _GLIBCXX_ALWAYS_INLINE bool _M_try_acquire_for(const chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) noexcept { auto const __pred = [this] { return _S_do_try_acquire(&amp;this-&gt;_M_counter); }; return __atomic_wait_address_for_bare(&amp;_M_counter, __pred, __rtime); } }; &lt;/pre&gt; &lt;p&gt;You might have observed that there are more &lt;code&gt;__atomic_wait_address_for&lt;/code&gt;/&lt;code&gt;until&lt;/code&gt; variations than are actually used by &lt;code&gt;__atomic_semaphore&lt;/code&gt;. C++26 will likely add timed versions of &lt;code&gt;wait()&lt;/code&gt; to &lt;code&gt;atomic&lt;/code&gt; as well as a version that accepts a predicate. The underlying support for these operations is already present but not yet exposed via the interface of &lt;code&gt;atomic,&lt;/code&gt; and the details are subject to change in a future version of GCC.&lt;/p&gt; &lt;h3&gt;The rest of &lt;semaphore&gt;&lt;/h3&gt; &lt;p&gt;The semaphore implementation to use is conditionally chosen based on the presence of the atomic wait feature-test macro:&lt;/p&gt; &lt;pre&gt; #if defined __cpp_lib_atomic_wait using __semaphore_impl = __atomic_semaphore; #elif _GLIBCXX_HAVE_POSIX_SEMAPHORE using __semaphore_impl = __platform_semaphore; #endif &lt;/pre&gt; &lt;p&gt;We then implement s&lt;code&gt;td::counting_semaphore&lt;/code&gt; in terms of &lt;code&gt;__semaphore_impl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; template&lt;ptrdiff_t __least_max_value = __semaphore_impl::_S_max&gt; class counting_semaphore { static_assert(__least_max_value &gt;= 0); static_assert(__least_max_value &lt;= __semaphore_impl::_S_max); __semaphore_impl _M_sem; public: explicit counting_semaphore(ptrdiff_t __desired) noexcept : _M_sem(__desired) { } ~counting_semaphore() = default; counting_semaphore(const counting_semaphore&amp;) = delete; counting_semaphore&amp; operator=(const counting_semaphore&amp;) = delete; static constexpr ptrdiff_t max() noexcept { return __least_max_value; } void release(ptrdiff_t __update = 1) noexcept(noexcept(_M_sem._M_release(1))) { _M_sem._M_release(__update); } void acquire() noexcept(noexcept(_M_sem._M_acquire())) { _M_sem._M_acquire(); } bool try_acquire() noexcept(noexcept(_M_sem._M_try_acquire())) { return _M_sem._M_try_acquire(); } template&lt;typename _Rep, typename _Period&gt; bool try_acquire_for(const std::chrono::duration&lt;_Rep, _Period&gt;&amp; __rtime) { return _M_sem._M_try_acquire_for(__rtime); } template&lt;typename _Clock, typename _Dur&gt; bool try_acquire_until(const std::chrono::time_point&lt;_Clock, _Dur&gt;&amp; __atime) { return _M_sem._M_try_acquire_until(__atime); } }; &lt;/pre&gt; &lt;p&gt;And &lt;code&gt;std::binary_semaphore&lt;/code&gt; is just a type alias:&lt;/p&gt; &lt;pre&gt; using binary_semaphore = std::counting_semaphore&lt;1&gt;; &lt;/pre&gt; &lt;h2&gt;Next time&lt;/h2&gt; &lt;p&gt;With all of the waiting detail and semaphores out of the way, the next installment will look at &lt;code&gt;&lt;latch&gt;&lt;/code&gt; and &lt;code&gt;&lt;barrier&gt;&lt;/code&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/18/implementing-c20-semaphores" title="Implementing C++20 semaphores"&gt;Implementing C++20 semaphores&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Thomas Rodgers</dc:creator><dc:date>2023-04-18T07:00:00Z</dc:date></entry><entry><title>My advice for transitioning to a clean architecture platform</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform" /><author><name>Maarten Vandeperre, Kevin Dubois</name></author><id>ddcab0d1-0a01-4520-9b72-a081a2a66062</id><updated>2023-04-17T07:01:00Z</updated><published>2023-04-17T07:01:00Z</published><summary type="html">&lt;p&gt;Now that you have become an application development expert, it’s time to look at how clean architecture would look when we map it to the infrastructure side of an &lt;a href="https://developers.redhat.com/app-dev-platform"&gt;application platform&lt;/a&gt;. In case you're not familiar with clean architecture, read part one in this series, &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture"&gt;My advice for building maintainable, clean architecture&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Preferred application platform architecture&lt;/h2&gt; &lt;p&gt;Before we get into the clean architecture mapping of our application platform, let’s describe how it should look (Figure 1).&lt;/p&gt; &lt;p&gt;We will have these four &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Personal data service&lt;/strong&gt;: Capturing personal information (i.e., email, name, phone number, age, gender, and address). Since this kind of data is relational in nature, we would opt for a relational database for this service.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movies catalog service&lt;/strong&gt;: A service to capture all movies from which we have collected data (i.e., name, director, actors, release date, and categories). We prefer to have this data stored in a NoSQL database.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movie tracking service&lt;/strong&gt;: A service to store tracking information about who watched a movie. Data is very limited (i.e., person ID, movie ID, timestamp, and if the movie was completed). We prefer to have this data stored in a NoSQL database.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Movie recommendation service&lt;/strong&gt;: A service that consumes data from the previous three microservices to provide recommendations based on viewer habits, age, gender, and broad location). Since this is connected data, we prefer to have this data stored in a graph database. Here too, we let go of DRY - data copied to the graph.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_1_service_outline.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_1_service_outline.jpeg?itok=xwmBanec" width="600" height="236" alt="An illustration of the microservices outline of preferred platform architecture" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The microservices outline of our preferred platform architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now that we have this outline, let's discuss how to implement it.&lt;/p&gt; &lt;h2&gt;AWS single cloud vendor&lt;/h2&gt; &lt;p&gt;Within company XYZ, developers played with AWS in the past. The management and architects supported the choice for AWS, so the development team started the design of the application platform on AWS cloud. The list of requirements to design their cloud infrastructure is as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;The requirement of the person microservice is that it should run on a SQL database. Within AWS, there are various options, but the development team chooses AWS Aurora.&lt;/li&gt; &lt;li aria-level="1"&gt;The requirement of the movie microservice is that it should run on a NoSQL database. AWS only offers DynamoDB off-the-shelf, so they choose DynamoDB.&lt;/li&gt; &lt;li aria-level="1"&gt;The movie-tracking microservice has the same requirements as the movie service; hence they choose DynamoDB.&lt;/li&gt; &lt;li aria-level="1"&gt;The requirement of the movie recommendation service is that it should run on a graph database. Since AWS only offers Neptune off-the-shelf (at the time of writing), they choose Neptune. If they need machine learning models, the development team can opt for AWS Athena or a machine learning model running on the graph database.&lt;/li&gt; &lt;li aria-level="1"&gt;The development team thinks that &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift on AWS (ROSA)&lt;/a&gt; is too expensive, so they choose the DIY solution provided by EKS or ECS (both &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; implementations of AWS), not thinking or knowing about the risks going along with this decision.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now the development team has designed their cloud infrastructure for the cloud application platform (Figure 2).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_2_general_design.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_2_general_design.jpeg?itok=zVVC7G-P" width="600" height="195" alt="An illustration of the design of the cloud architecture for the application platform." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The design of the cloud architecture for the application platform.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In the next section, they will get back to the management team and the architects.&lt;/p&gt; &lt;h2&gt;The evaluation of dependencies&lt;/h2&gt; &lt;p&gt;In this section, we’ll look at the AWS services as if they were code dependencies. Instead of libraries referenced from the code (see our &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture"&gt;previous article&lt;/a&gt;), these dependencies will be cloud services referenced from the microservices. If we refer to clean architecture, the four microservices are the core-layer (i.e., the business logic that should stand the test of time) and the AWS services are the dependencies that should be easy to replace. We should not tightly couple (combine) with any specific AWS service (as interface layer, &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;Camel&lt;/a&gt; can be used, but more on that in future articles). There is one exception. EKS or ECS is the hosting infrastructure and can be seen as the programming language within clean architecture.&lt;/p&gt; &lt;p&gt;The management team and the architects looked at the design of the application platform’s cloud architecture and had the following recommendations (Figure 3):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Usage of AWS Aurora:&lt;/strong&gt; That’s fine. It’s recommended by AWS. It’s a stable solution. This choice is accepted (acceptance rate: green).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of DynamoDB&lt;/strong&gt;: This choice is not well received. The reason is that they bought or installed an on-premise bare-metal MongoDB cluster last year. In order to have some ROI, the management team and the architects prefer to use this cluster. If there is no other choice, DynamoDB can be accepted (acceptance rate: orange).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of Neptune:&lt;/strong&gt; This is a no-go. Although it’s provided off-the-shelf by AWS, it’s not the best graph database on the market (at the time of writing). The management team and the architects would prefer Neo4J or TigerGraph (acceptance rate: red).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Usage of EKS or ECS:&lt;/strong&gt; Accepted by the management team and the architects, but this discussion is only won on the short-term pricing argument. They may have thought that OpenShift on AWS is too expensive, but they forgot the risks and the hidden costs that go along with a DIY solution.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_3_evaluation.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_3_evaluation.jpeg?itok=ZgMY1Ge9" width="600" height="195" alt="An illustration of the evaluation of the dependencies." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The evaluation of the dependencies.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Because not everything was accepted, the development team must go back to the drawing board.&lt;/p&gt; &lt;h2&gt;Preferred dependencies&lt;/h2&gt; &lt;p&gt;The development team, good listeners as they are, take the feedback from the management team and architects into account and start adapting their architecture. Their preferred dependencies are as follows (Figure 4):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;AWS Aurora was accepted. So AWS Aurora stays.&lt;/li&gt; &lt;li aria-level="1"&gt;Management was not too happy with the lack of ROI on their investment in the bare-metal MongoDB cluster. If they don’t want to have repercussions on future requested investments, it’s maybe a good idea to reuse that cluster, which is what they’ll do. AWS DynamoDB will be replaced by the on-premise bare-metal MongoDB cluster.&lt;/li&gt; &lt;li aria-level="1"&gt;Neptune was a no-go. The development team invests some time in reviewing graph databases. The winner in this exercise is Neo4J, off-the-shelf, offered by Google Cloud. AWS Neptune will be replaced by Google Cloud Neo4J.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_4_add_preferred_dependencies.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_4_add_preferred_dependencies.jpeg?itok=qpsizAp5" width="600" height="189" alt="An illustration adding the preferred dependencies to the architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Adding the recommended or preferred dependencies to the architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All of these changes should go relatively easy because the development team follows the principles of clean architecture. But it does not go easy. EKS or ECS has issues connecting to our on-premise MongoDB cluster and Google Cloud Neo4J (unless they spend quite some time and effort in fixing the network setup, which would even open the door for security flaws, as the development team is less trained in networking and operations).&lt;/p&gt; &lt;p&gt;The development team does not know how to fix the cloud (i.e., core layer) issue regarding the dependencies and asks the architects for input. This will be covered in the next section.&lt;/p&gt; &lt;p&gt;There is an analogy that can be made with &lt;a href="http://developers.redhat.com/topics/java"&gt;Java&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Imagine that person service is a Java service and that the on-premise MongoDB is a Python library. If the programming language (like EKS or ECS) is Java, you won’t be able to access the Python libraries from within the person service. If you change the programming language to JVM and then to GraalVM, it would allow Java code to access Python scripts. It then would allow that the movie and movie tracking service are Python scripts (i.e., allow using these libraries) and that they can be called from within a Java service. Changing the programming language from Java to GraalVM, opened the options to a broader spectrum of third-party libraries.&lt;/p&gt; &lt;h2&gt;Hybrid cloud and multicloud solutions to the rescue&lt;/h2&gt; &lt;p&gt;The development team consults the architects with the issue: “We have isolated our dependencies (i.e., cloud services), but have issues with organizing our core layer (i.e., the third-party independent business logic, the microservices).” The architects look to see where the issue originated.&lt;/p&gt; &lt;p&gt;Clean architecture is about keeping your options open. The development team did this, but they narrowed the options too much by limiting their programming language (i.e., the microservice hosting platform) to AWS only. What they really need to make full use of the options available in the cloud and on-premise, is a hybrid cloud and multicloud. With a hybrid cloud, we mean the combination of a part of the cloud located at a vendor and a part of the cloud located on-premise. With a multicloud, we mean that the cloud is located over multiple vendors.&lt;em&gt; &lt;/em&gt;When hybrid cloud and multicloud solutions are enabled as programming language, the development team will be able to use the preferred dependencies: AWS Aurora, on-premise MongoDB, and Google Cloud Neo4J.&lt;/p&gt; &lt;p&gt;This architecture will withstand the test of time as well. Whenever a new SaaS solution becomes available or a new cloud emerges, or new tooling is needed, it will be easy to plug it in the designed cloud architecture (Figure 5). This results in a more resilient platform that requires less maintenance and that fairly easily allows for future innovation.&lt;strong&gt; &lt;/strong&gt;&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/step_5_hybrid_and_multi_cloud.jpeg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/step_5_hybrid_and_multi_cloud.jpeg?itok=YSx4igPs" width="600" height="201" alt="An illustration of the hybrid cloud and multicloud options." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Maarten Vandeperre&lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Hybrid cloud and multicloud.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But how can they implement a hybrid cloud and multicloud solution?&lt;/p&gt; &lt;h2&gt;How to set up hybrid cloud and multicloud solutions&lt;/h2&gt; &lt;p&gt;How do you set up hybrid cloud or multicloud solutions? &lt;a href="www.redhat.com/en/resources/openshift-container-platform-datasheet"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; is the answer. It is a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; platform that has the ability to easily start building hybrid cloud and/or multicloud solutions.&lt;/p&gt; &lt;p&gt;Although we used OpenShift as a solution for the issues that come along with EKS, OpenShift is more than just a Kubernetes installation or implementation. OpenShift Container Platform is a full application container platform. Check with the OpenShift specialists about how to bring the hybrid cloud and multi-cloud service to your company. Get started with a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Service on AWS (ROSA) trial&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform" title="My advice for transitioning to a clean architecture platform"&gt;My advice for transitioning to a clean architecture platform&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maarten Vandeperre, Kevin Dubois</dc:creator><dc:date>2023-04-17T07:01:00Z</dc:date></entry><entry><title>My advice for building maintainable, clean architecture</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture" /><author><name>Maarten Vandeperre, Kevin Dubois</name></author><id>cd4e3633-e78e-4cbd-b4b1-669cf87598f3</id><updated>2023-04-17T07:00:00Z</updated><published>2023-04-17T07:00:00Z</published><summary type="html">&lt;p&gt;To say that &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; is an illusion is a controversial statement to start this article. What I mean is that I often see DevOps passing by, but I have the feeling that they forgot the “and.” Development and operations are often two separated silos, not looking at each other, not looking at each other’s principles, even in cloud development. One of the best examples of looking for best practices in other silos is (in my opinion) agile software development, which originated from the Japanese car manufacturer, Toyota.&lt;/p&gt; &lt;p&gt;As hybrid and/or multi-cloud is often positioned from an infrastructure point of view, I believe that it can bring added value to tell the story about it, seeing through “application development glasses" instead of “infrastructure glasses." Hence, we wrote this article.&lt;/p&gt; &lt;p&gt;This is part one of two articles in a series about clean architecture. Part two covers &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform"&gt;transitioning to a clean architecture platform&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Knowing versus understanding&lt;/h2&gt; &lt;p&gt;Everybody knows mathematics, but not everybody understands integrals or knows how to apply volume calculations. Everybody knows that the surface of a triangle is ((b x h) / 2), but not everybody knows how to come to this formula. Some developers know the concept of clean architecture but don’t understand the internal details or principles enough to apply it to other domains (e.g., cloud infrastructure and cloud platform setup).&lt;/p&gt; &lt;p&gt;We often look at &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; with infrastructure glasses, but development-oriented people care less about security, ease to set up the cluster(s). Next to that, infrastructure-oriented people are less interested in application development best practices, while there can be or should be a synergy between the two worlds.&lt;/p&gt; &lt;h3&gt;How to handle data&lt;/h3&gt; &lt;p&gt;Consider the concepts of DRY vs. WRYMSIU (i.e., while repeating yourself, make sure it’s useful). Don’t repeat yourself. Personally, I think this statement is outdated and should be replaced with WRYMSIU (I apologize, I’m not the best inventor of acronyms). This means that you can repeat yourself (e.g., data), but only if the duplication of data brings added value (like we will see later).&lt;/p&gt; &lt;p&gt;An example of this is an analytics engine that runs in another environment than where your result-showing application is living. If application performance is a priority, then it can bring added value to copy the result-data from the environment of the analytics engine to the application environment. Nowadays, storage is not that expensive anymore, which leads to my opinion that DRY is outdated.&lt;/p&gt; &lt;h3&gt;Java application setup&lt;/h3&gt; &lt;p&gt;There are a lot of options in dependencies and languages, but in order to set up a project or application, you need to work in the scope or context of a programming language. This is the only choice that you should be or will be tied to.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Dependencies: &lt;/strong&gt;Libraries or frameworks that are provided by third parties and that offer some functionality (e.g., logging, connection to Oracle, MongoDB, MySQL, databases, framework to expose REST/GraphQL endpoints). They are linked to your code base and can then be included within your source code (see Image 1).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOA&lt;/strong&gt;: Service-oriented architecture. Controller (i.e., entry point), service layer (i.e., business logic), repository layer (i.e., database layer and persistence layer). Issues with SOA include the concept of service is error-prone (i.e., updating the code base to introduce workflow x can break workflow y). In this analogy, serverless function, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Jobs should be preferred over real applications, but out of scope for this presentation.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Focus lies on dependencies from now on, with the choice for &lt;a href="http://developers.redhat.com/topics/java"&gt;Java&lt;/a&gt; as programming language.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1: Pollution of data classes (i.e., domain entities)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The issue with this way of coding is the database layer is directly intertwined with the domain entity (Figure 1). This results in that you have to know which type of database you’ll be using at the time of writing your data classes, and that whenever you would like to change your database technology, you’ll need to change the application core (i.e., business logic and application behavior).&lt;/p&gt; &lt;p&gt;A third issue with such code is the fact that data validation is handled by the library, based upon annotations. When you would try a new database technology, and you forget that validation should be rewritten now, the entire data validation will be compromised. Quite a lot of risks to just try out or change a library or database layer.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/code_annotations.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/code_annotations.png?itok=eYRl5AEI" width="450" height="587" alt="Import of dependencies and hard link with database layer and annotations in code." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt;&lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/spring-projects/spring-petclinic&lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Import of dependencies and hard link with database layer and annotations in code.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Example 2: Dependencies in the core (i.e., service) layer&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/kozmer/log4j-shell-poc/blob/main/vulnerable-application/src/main/java/com/example/log4shell/LoginServlet.java"&gt;Log4J shell POC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Issue with this way of coding: This simple business logic (Figure 2) is tightly coupled to the Apache log4j library. Whenever you now would like to use another logging framework/library, you’ll have to refactor/touch a lot of classes, and you’ll have to touch the business logic layer. Hence, you’ll have a risk of introducing bugs in your application’s behavior, just by changing a library. This should never be the case, and this is something what implementing clean architecture (properly) would prevent.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/code_logger.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/code_logger.png?itok=aomva3sv" width="600" height="510" alt="Java code with library dependency to support logging functionality using third-party dependencies in code." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt;&lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/spring-projects/spring-petclinic&lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Java code with library dependency to support logging functionality using third-party dependencies in code.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Both examples showed that depending on third-party libraries in your core/service/business logic layer can result in the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Higher maintenance costs and cycles&lt;/strong&gt; (i.e., hard to keep dependencies up-to-date).&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Higher risk&lt;/strong&gt; when changing/upgrading dependencies.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Less innovation&lt;/strong&gt;: It takes some time to change dependencies, and it comes with risks. Not too much enthusiasm to play around with new technologies or dependencies.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This is something that implementing clean architecture properly would prevent. Clean architecture can still cause bugs during refactoring, but business logic should not break.&lt;/p&gt; &lt;h2&gt;What is clean architecture?&lt;/h2&gt; &lt;p&gt;One of the main principles of clean architecture is nothing more than keeping your options open by changing, adding, or removing dependencies as often as you want. We offer two analogies to illustrate this point.&lt;/p&gt; &lt;h3&gt;Real world analogies&lt;/h3&gt; &lt;p&gt;If you’re in the lead of a football team, you better not offer long term contracts to a new coach: (maybe this is an exaggeration) because in 90% of the cases, a coach doesn’t last for more than two years. Offering short term contracts, will allow the team to change trainers as often as we think is needed to achieve good results.&lt;/p&gt; &lt;p&gt;As a business analogy, companies that do not focus on software development (e.g., companies with a focus towards biochemistry) often rely on consultancy companies. Again, to keep options open. When they need more software developers, want to change software developers (e.g., mismatch with the atmosphere within the team) or just want to decrease the amount of developers (e.g., wrapping up of a project), then they are more flexible to do so when dealing with consultants than when they’d be dealing with internal employees.&lt;/p&gt; &lt;h3&gt;Technical explanation of clean architecture&lt;/h3&gt; &lt;p&gt;A good demo project can be found on &lt;a href="https://github.com/mattia-battiston/clean-architecture-example"&gt;GitHub by Mattia Battiston&lt;/a&gt;. I often start with this project to do my talks.&lt;/p&gt; &lt;p&gt;Clean architecture is about moving all your infrastructure and dependencies to the outer layer of your code base, leaving the business logic (domain entities and use cases, but these concepts are out of scope for this document/presentation) in the core (Figures 3 and 4). This has as an added value that it’s fairly cheap to change infrastructure, dependencies, or libraries as they are in the outer layer. Moving from &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; to &lt;a href="https://developers.redhat.com/node/219015"&gt;Quarkus&lt;/a&gt;, replacing Oracle with MySQL, ... will be easier to do. When trying something new is fairly cheap, then development teams can work around proof-of-concepts regarding modernizing their code base/application platform driving innovation&lt;strong&gt; &lt;/strong&gt;(i.e., goes hand-in-hand with the values of Red Hat).&lt;/p&gt; &lt;p&gt;Last but not least, making it easy to change dependencies results in easier dependency upgrades without the risk of breaking the business logic. Hence, the core-layer (i.e., the business logic) is relatively easy to maintain and should be able to live for a longer period of time&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Infrastructure and dependencies often go hand-in-hand as every infrastructure component is managed by a third-party library. This is the case for REST/GraphQL APIs, databases, jobs, and file system access and network calls (although basic functionality to do so, is embedded in Java since Java 11).&lt;br /&gt;&lt;br /&gt; As it’s not the focus of this article to do a deep dive about clean architecture, we’ll go for now with the concept of clean architecture is abstracting away business logic, the core of your business, from it’s (external, infrastructural) dependencies to avoid the marriage between infrastructure and dependencies&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As a side note (but beyond the scope of this article), going from SOA to use cases, makes you let go of DRY (for a valid reason in my opinion).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/clean_architecture_hexagonal.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/clean_architecture_hexagonal.png?itok=CD8cYErC" width="600" height="450" alt="Clean Architecture - Hexagonal Visual" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/mattia-battiston/clean-architecture-example&lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Clean architecture (V1) - Clean Architecture - Hexagonal Visual&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/clean_architecture_uncle_bob.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/clean_architecture_uncle_bob.png?itok=TU2pF9cx" width="600" height="450" alt="Clean Architecture - Uncle Bob's Visual" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-source-url field--type-string field--label-hidden field__items"&gt; https://github.com/mattia-battiston/clean-architecture-example&lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Clean architecture (V2) - Clean Architecture - Uncle Bob's Visual&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Learn more about clean architecture&lt;/h2&gt; &lt;p&gt;In this article, I shared my opinions and advice about the benefits of building maintainable, clean architecture. If you want to learn more about clean architecture, feel free to reach out to me (but reserve some time because I’m passionate about it).&lt;/p&gt; &lt;p&gt;Be sure to check out the &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-transitioning-clean-architecture-platform"&gt;next article&lt;/a&gt; discussing the transition from functional &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; to a clean architecture platform. If you have any questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/17/my-advice-building-maintainable-clean-architecture" title="My advice for building maintainable, clean architecture"&gt;My advice for building maintainable, clean architecture&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maarten Vandeperre, Kevin Dubois</dc:creator><dc:date>2023-04-17T07:00:00Z</dc:date></entry><entry><title type="html">Byteman 4.0.21 has been released</title><link rel="alternate" href="http://bytemanblog.blogspot.com/2023/04/byteman-4021-has-been-released.html" /><author><name>Andrew Dinn</name></author><id>http://bytemanblog.blogspot.com/2023/04/byteman-4021-has-been-released.html</id><updated>2023-04-14T10:55:00Z</updated><content type="html">  Byteman 4.0.21 is now available from the and from the . It is the latest update release for use on all JDK9+ runtimes up to and including JDK21.   Byteman 4.0.21 is a maintenance release which enables Byteman to be used with JDK21 releases. It also contains one  small bug fixes and a feature enhancement. More details are provided in the and the latest .</content><dc:creator>Andrew Dinn</dc:creator></entry><entry><title>How to deploy Open Policy Agent for API authorization</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/13/how-deploy-open-policy-agent-api-authorization" /><author><name>Jorge Balderas</name></author><id>56531825-292e-470c-866c-833c3e9af210</id><updated>2023-04-13T07:00:00Z</updated><published>2023-04-13T07:00:00Z</published><summary type="html">&lt;p&gt;In this article, we will demonstrate how to deploy Open Policy Agent in server mode into a &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; cluster. We will then set up simple Rego policies to validate a JWT token and provide authorization to specific APIs.&lt;/p&gt; &lt;h2&gt;About Open Policy Agent&lt;/h2&gt; &lt;p&gt;Open Policy Agent (&lt;a href="https://www.openpolicyagent.org/docs/latest/" target="_blank"&gt;OPA&lt;/a&gt;) is a Cloud Native Computing Foundation (CNCF) graduated project. OPA is an open source policy agent ideal for decoupling authorization from cloud-native applications, APIs, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; resources, and &lt;a href="http://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; pipelines, along with other artifacts. OPA uses &lt;a href="https://www.openpolicyagent.org/docs/latest/policy-language/" target="_blank"&gt;Rego&lt;/a&gt;, a declarative language, to define policies as code. Applications and services can use OPA to query, provide an input evaluated against predefined policies, and provide a policy decision, as shown in the Figure 1 diagram.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opa-flow.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/opa-flow.png?itok=KFORdeWg" width="600" height="450" alt="A diagram depicting the flow for evaluating policies by Open Policy Agent." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The flow for evaluating policies by Open Policy Agent.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;How to deploy OPA using REST API&lt;/h2&gt; &lt;p&gt;OPA provides 3 primary &lt;a href="https://www.openpolicyagent.org/docs/latest/integration/#comparison" target="_blank"&gt;options&lt;/a&gt; of deploying OPA to evaluate policies:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;&lt;strong&gt;REST API:&lt;/strong&gt; Deployed separate from your application or service.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Go library:&lt;/strong&gt; Requires Go to deploy as a side car alongside your application.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WebAssembly (WASM):&lt;/strong&gt; Deployed alongside your application regardless of the language.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;In this article, we will demonstrate the REST API option and focus on the use cases for leveraging OPA for API authorization. We will describe each step for deploying OPA, creating simple policies, and then evaluating the using the OPA REST API.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;p&gt;To complete this demo you will need the following prerequisites:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Red Hat OpenShift cluster:&lt;/strong&gt; For this demo, we use OCP 4.12. However, any 4.x version should work. Alternatively, you can use any Kubernetes cluster. The instructions in this demo are specific to OpenShift.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenShift CLI:&lt;/strong&gt; You can download the OpenShift CLI (&lt;code&gt;oc&lt;/code&gt;) from your cluster as specified in the &lt;a href="https://docs.openshift.com/container-platform/4.12/cli_reference/openshift_cli/getting-started-cli.html" target="_blank"&gt;OpenShift documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Git bash or shell/bash terminal:&lt;/strong&gt; Although not required, the commands presented in this article assume a Linux shell syntax.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Deploying OPA in 2 steps&lt;/h3&gt; &lt;p&gt;To deploy OPA as a REST API, first create three Kubernetes resources.&lt;/p&gt; &lt;h4&gt;Step 1: Create Kubernetes definition files&lt;/h4&gt; &lt;p&gt;Create a file named &lt;strong&gt;deployment.yaml&lt;/strong&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: opa spec: replicas: 1 selector: matchLabels: app: opa template: metadata: labels: app: opa spec: containers: - name: opa securityContext: capabilities: drop: ["ALL"] runAsNonRoot: true allowPrivilegeEscalation: false seccompProfile: type: "RuntimeDefault" image: openpolicyagent/opa:0.50.1-debug args: - "run" - "--watch" - "--ignore=.*" - "--server" - "--skip-version-check" - "--log-level" - "debug" - "--set=status.console=true" - "--set=decision_logs.console=true" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Most of these parameters are optional, but we included them for higher verbosity level which is helpful for troubleshooting. You can view the purpose of these parameters in the documentation for the &lt;a href="https://www.openpolicyagent.org/docs/latest/cli/#opa-run" target="_blank"&gt;&lt;strong&gt;opa run&lt;/strong&gt;&lt;/a&gt; command.&lt;/p&gt; &lt;p&gt;Important observations:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;We are using OPA version 0.50.1, the latest available at the time of writing. We are using the &lt;code&gt;-debug&lt;/code&gt; version of the OPA image which includes a CLI that can be useful for inspecting the deployed files. However for a production release it is recommended that you use the &lt;code&gt;-rootless&lt;/code&gt; version of this image.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;--server&lt;/code&gt; parameter is what tells OPA to run in server mode so that it can listen for REST API requests.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Next, create a file named &lt;strong&gt;service.yaml&lt;/strong&gt; which will expose OPA as a service within the cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: Service apiVersion: v1 metadata: labels: app: opa name: opa spec: selector: app: opa ports: - name: http protocol: TCP port: 80 targetPort: 8181&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, create a file named &lt;strong&gt;route.yaml&lt;/strong&gt; which will expose OPA service as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kind: Route apiVersion: route.openshift.io/v1 metadata: labels: app: opa name: opa spec: selector: matchLabels: app: opa to: kind: Service name: opa port: targetPort: http&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We are using http for demo purposes. In a production environment, ensure that you are using https.&lt;/p&gt; &lt;h4&gt;Step 2: Deploy Kubernetes resources to OpenShift cluster&lt;/h4&gt; &lt;p&gt;First, log in to your OpenShift cluster by obtaining a token from your cluster using the OC CLI as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc login --token=&lt;sha256~token&gt; --server=&lt;your-openshift-cluster-api-url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a new project for this cluster. We export the &lt;code&gt;NAMESPACE&lt;/code&gt; name as a variable so that it can be used in subsequent steps.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAMESPACE=opa oc new-project $NAMESPACE&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create the resources using the files created previously as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -f deployment.yaml -n $NAMESPACE oc apply -f service.yaml -n $NAMESPACE oc apply -f route.yaml -n $NAMESPACE&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the route that was created, issue the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;echo http://$(oc get route $NAMESPACE -o jsonpath='{.spec.host}')&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate to the route in your browser to view the OPA home screen (Figure 2), which allows you to evaluate a policy.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opa-home.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/opa-home.png?itok=cqmISo-t" width="600" height="742" alt="A screenshot of the OPA home screen." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The OPA home screen.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Policy evaluation 3-step demo&lt;/h3&gt; &lt;p&gt;Now, we need to define and load policies for demo purposes.&lt;/p&gt; &lt;h4&gt;Step 1: Create common JWT policy&lt;/h4&gt; &lt;p&gt;One of the nice features about Rego is that it provides several &lt;a href="https://www.openpolicyagent.org/docs/latest/policy-reference/#built-in-functions" target="_blank"&gt;built-in functions&lt;/a&gt;. One set of functions that is particularly helpful is the one for JWT (JSON Web Token) &lt;a href="https://www.openpolicyagent.org/docs/latest/policy-reference/#tokens" target="_blank"&gt;token validation&lt;/a&gt;. The policy will decode a JWT token, and then validate it against the secret used to sign the token.&lt;/p&gt; &lt;p&gt;We will use a shared secret for demo purposes. However, the JWT function can verify the token using &lt;a href="https://redthunder.blog/2017/06/08/jwts-jwks-kids-x5ts-oh-my/" target="_blank"&gt;JWKS&lt;/a&gt; (JSON Web Key Sets). Anybody familiar with the JWKS verification flow knows that it is not a trivial implementation. The built-in verify token functions will take care of retrieving KIDs (key ids) from the corresponding well known URL, and it even provides caching capabilities to speed up that process.&lt;/p&gt; &lt;p&gt;First, create a file named &lt;code&gt;jwt.rego&lt;/code&gt;as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;package com.redhat.common.jwt import input import future.keywords.in valid_token(jwt) = token { [header, payload, sig]:= io.jwt.decode(jwt) valid := io.jwt.verify_hs256(jwt, 'secret') token := {"valid": valid, "name": payload.name} }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see from this Rego file, it is primarily JSON, except for the import/package headers. Again, we are using a shared secret, which is done only for demo purposes.&lt;/p&gt; &lt;p&gt;Next, load this policy using the &lt;a href="https://www.openpolicyagent.org/docs/latest/rest-api/#create-or-update-a-policy" target="_blank"&gt;create policy&lt;/a&gt; REST API from the OPA as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;OPA_URL=http://$(oc get route $NAMESPACE -o jsonpath='{.spec.host}') cat jwt.rego | curl --location --request PUT "${OPA_URL}/v1/policies/com/redhat/common/jwt" --header 'Content-Type: text/plain' --data-binary '@-'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Deconstructing this URL:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;${OPA_URL}&lt;/code&gt;: The base OPA URL.&lt;/li&gt; &lt;li&gt;&lt;code&gt;v1/policies&lt;/code&gt;: The default location for policies.&lt;/li&gt; &lt;li&gt;&lt;code&gt;com/redhat/common/jwt&lt;/code&gt;: This is how policies are retrieved. Notice that it matches the package name (i.e., &lt;code&gt;com.redhat.common.jwt&lt;/code&gt;), but using a different character separator. There is no hard rule that these should match, but I have found it a good practice to follow to make it easier to organize policies.&lt;/li&gt; &lt;/ul&gt;&lt;h4&gt;Step 2: Create API authorization policy&lt;/h4&gt; &lt;p&gt;In this step, we will create a policy that uses the common JWT policy loaded in step 1. Create a file named &lt;code&gt;api.rego&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;package com.redhat.myapi import data.com.redhat.common.jwt.valid_token default allow := { #disallow requests by default "allowed": false, "reason": "unauthorized resource access" } allow := { "allowed": true } { #allow GET requests to viewer user input.method == "GET" input.path[1] == "policy" token := valid_token(input.identity) token.name == "viewer" token.valid } allow := { "allowed": true } { #allow POST requests to admin user input.method == "POST" input.path[1] == "policy" token := valid_token(input.identity) token.name == "admin" token.valid }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the import to the &lt;code&gt;valid_token&lt;/code&gt; function. It matches the package used previously, but it is prepended with &lt;code&gt;data&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Next, load this policy with a similar &lt;code&gt;curl&lt;/code&gt; command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat api.rego | curl --location --request PUT "${OPA_URL}/v1/policies/com/redhat/myapi" --header 'Content-Type: text/plain' --data-binary '@-'&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Step 3: Evaluate the policy&lt;/h4&gt; &lt;p&gt;To evaluate the policy, we will need to get a valid JWT token. You can get one from &lt;a href="http://jwt.io" target="_blank"&gt;jwt.io&lt;/a&gt;, the only requirement is that you enter the same secret from the &lt;strong&gt;jwt&lt;/strong&gt; policy into the &lt;strong&gt;&lt;your-256-bit-secret&gt;&lt;/strong&gt; in the &lt;strong&gt;Verify Signature&lt;/strong&gt; section.&lt;/p&gt; &lt;p&gt;Additionally, change the name in the &lt;strong&gt;Payload&lt;/strong&gt; section to &lt;code&gt;viewer&lt;/code&gt; and copy the generated token.&lt;/p&gt; &lt;p&gt;Repeat these steps and enter &lt;strong&gt;admin&lt;/strong&gt; as the name, and then save both tokens in a file from where you can copy values.&lt;/p&gt; &lt;p&gt;Next, create a request to test a successful viewer request named &lt;code&gt;viewer-allowed.json&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "input": { "identity": "&lt;viewer token&gt;", "path": "policy", "method": "GET" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the curl command (notice the url changes from &lt;strong&gt;policy&lt;/strong&gt; to &lt;strong&gt;data&lt;/strong&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat viewer-not-allowed.json | curl --location --request POST "${OPA_URL}/v1/data/com/redhat/myapi" --header 'Content-Type: application/json' --data-binary '@-'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Expect an &lt;strong&gt;allowed true&lt;/strong&gt; output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "result": { "allow": { "allowed": true } } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a request to test a not allowed viewer request named &lt;code&gt;viewer-not-allowed.json&lt;/code&gt; by changing the method to &lt;strong&gt;POST.&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "input": { "identity": "&lt;viewer token&gt;", "path": "policy", "method": "POST" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the following curl command and expect the output to include &lt;strong&gt;allowed false&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;cat viewer-not-allowed.json | curl --location --request POST "${OPA_URL}/v1/data/com/redhat/myapi" --header 'Content-Type: application/json' --data-binary '@-' {"result":{"allow":{"allowed":false,"reason":"unauthorized resource access"}}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create an &lt;code&gt;admin-allowed.json&lt;/code&gt; file with the following request:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "input": { "identity": "&lt;admin jwt token&gt;", "path": "policy", "method": "POST" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute the curl command and expect the output to include &lt;strong&gt;allowed true&lt;/strong&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat admin-allowed.json | curl --location --request POST "${OPA_URL}/v1/data/com/redhat/myapi" --header 'Content-Type: application/json' --data-binary '@-'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Open Policy Agent easily deployed&lt;/h2&gt; &lt;p&gt;This article demonstrated how to easily deploy the Open Policy Agent into an OpenShift cluster and load a common JWT policy with an API policy. We also described how policy evaluation works within OPA. This demo showcased a small set of the capabilities and potential that OPA offers, providing an introduction to OPA and Rego policies.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/13/how-deploy-open-policy-agent-api-authorization" title="How to deploy Open Policy Agent for API authorization"&gt;How to deploy Open Policy Agent for API authorization&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jorge Balderas</dc:creator><dc:date>2023-04-13T07:00:00Z</dc:date></entry><entry><title>Why you should use io_uring for network I/O</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/12/why-you-should-use-iouring-network-io" /><author><name>Donald Hunter</name></author><id>0b396414-89e5-49a9-8932-b7604cb3ba2d</id><updated>2023-04-12T07:00:00Z</updated><published>2023-04-12T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;code&gt;io_uring&lt;/code&gt; is an async interface to the &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; kernel that can potentially benefit networking. It has been a big win for file I/O (input/output), but might offer only modest gains for network I/O, which already has non-blocking APIs. The gains are likely to come from the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;A reduced number of syscalls on servers that do a lot of context switching&lt;/li&gt; &lt;li aria-level="1"&gt;A unified asynchronous API for both file and network I/O&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Many &lt;code&gt;io_uring&lt;/code&gt; features will soon be available in &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9.3, which is distributed with kernel version 5.14. The latest &lt;code&gt;io_uring&lt;/code&gt; features are currently available in Fedora 37.&lt;/p&gt; &lt;h2&gt;What is io_uring?&lt;/h2&gt; &lt;p&gt;&lt;code&gt;io_uring&lt;/code&gt; is an asynchronous I/O interface for the Linux kernel. An &lt;code&gt;io_uring&lt;/code&gt; is a pair of ring buffers in shared memory that are used as queues between user space and the kernel:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Submission queue (SQ): A user space process uses the submission queue to send asynchronous I/O requests to the kernel.&lt;/li&gt; &lt;li&gt;Completion queue (CQ): The kernel uses the completion queue to send the results of asynchronous I/O operations back to user space.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The diagram in Figure 1 shows how &lt;code&gt;io_uring&lt;/code&gt; provides an asynchronous interface between user space and the Linux kernel.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/uring_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/uring_0.png?itok=kNKFe-On" width="600" height="397" alt="Two ring buffers called the submission queue and the completion queue. An application is adding an item to the tail of the submission queue and the kernel is consuming an item from the head of the submission queue. The completion queue shows the reverse for responses from kernel to application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-creator field--type-string field--label-inline field__items"&gt; &lt;span class="field__label"&gt;Creator&lt;/span&gt; &lt;span class="rhd-media-creator field__item"&gt;Donald Hunter&lt;/span&gt; &lt;/span&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: A visual representation of the io_uring submission and completion queues.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This interface enables applications to move away from the traditional readiness-based model of I/O to a new completion-based model where async file and network I/O share a unified API.&lt;/p&gt; &lt;h2&gt;The syscall API&lt;/h2&gt; &lt;p&gt;The Linux kernel API for &lt;code&gt;io_uring&lt;/code&gt; has 3 syscalls:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;io_uring_setup&lt;/code&gt;: Set up a context for performing asynchronous I/O&lt;/li&gt; &lt;li&gt;&lt;code&gt;io_uring_register&lt;/code&gt;: Register files or user buffers for asynchronous I/O&lt;/li&gt; &lt;li&gt;&lt;code&gt;io_uring_enter&lt;/code&gt;: Initiate and/or complete asynchronous I/O&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The first two syscalls are used to set up an &lt;code&gt;io_uring&lt;/code&gt; instance and optionally to pre-register buffers that would be referenced by &lt;code&gt;io_uring&lt;/code&gt; operations. Only &lt;code&gt;io_uring_enter&lt;/code&gt; needs to be called for queue submission and consumption. The cost of an &lt;code&gt;io_uring_enter&lt;/code&gt; call can be amortized over several I/O operations. For very busy servers, you can avoid &lt;code&gt;io_uring_enter&lt;/code&gt; calls entirely by enabling busy-polling of the submission queue in the kernel. This comes at the cost of a kernel thread consuming CPU.&lt;/p&gt; &lt;h2&gt;The liburing API&lt;/h2&gt; &lt;p&gt;The liburing library provides a convenient way to use &lt;code&gt;io_uring&lt;/code&gt;, hiding some of the complexity and providing functions to prepare all types of I/O operations for submission.&lt;/p&gt; &lt;p&gt;A user process creates an &lt;code&gt;io_uring&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct io_uring ring; io_uring_queue_init(QUEUE_DEPTH, &amp;ring, 0);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then submits operations to the &lt;code&gt;io_uring&lt;/code&gt; submission queue:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct io_uring_sqe *sqe = io_uring_get_sqe(&amp;ring); io_uring_prep_readv(sqe, client_socket, iov, 1, 0); io_uring_sqe_set_data(sqe, user_data); io_uring_submit(&amp;ring);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The process waits for completion:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;struct io_uring_cqe *cqe; int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and uses the response:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;user_data = io_uring_cqe_get_data(cqe); if (cqe-&gt;res &lt; 0) {     // handle error } else {     // handle response } io_uring_cqe_seen(&amp;ring, cqe);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The liburing API is the preferred way to use &lt;code&gt;io_uring&lt;/code&gt; from applications. liburing has feature parity with the latest kernel &lt;code&gt;io_uring&lt;/code&gt; development work and is backward-compatible with older kernels that lack the latest &lt;code&gt;io_uring&lt;/code&gt; features.&lt;/p&gt; &lt;h2&gt;Using io_uring for network I/O&lt;/h2&gt; &lt;p&gt;We will try out &lt;code&gt;io_uring&lt;/code&gt; for network I/O by writing a simple echo server using the liburing API. Then we will see how to minimize the number of syscalls required for a high-rate concurrent workload.&lt;/p&gt; &lt;h3&gt;A simple echo server&lt;/h3&gt; &lt;p&gt;The classic echo server that appeared in Berkeley Software Distribution (BSD) Unix looks something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;client_fd = accept(listen_fd, &amp;client_addr, &amp;client_addrlen); for (;;) {     numRead = read(client_fd, buf, BUF_SIZE);     if (numRead &lt;= 0)   // exit loop on EOF or error         break;     if (write(client_fd, buf, numRead) != numRead)         // handle write error     } } close(client_fd);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The server could be multithreaded or use non-blocking I/O to support concurrent requests. Whatever form it takes, the server requires at least 5 syscalls per client session, for accept, read, write, read to detect EOF and then close.&lt;/p&gt; &lt;p&gt;A naive translation of this to &lt;code&gt;io_uring&lt;/code&gt; results in an asynchronous server that submits one operation at a time and waits for completion before submitting the next. The pseudocode for a simple &lt;code&gt;io_uring&lt;/code&gt;-based server, omitting the boilerplate and error handling, looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;add_accept_request(listen_socket, &amp;client_addr, &amp;client_addr_len); io_uring_submit(&amp;ring); while (1) {     int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);     struct request *req = (struct request *) cqe-&gt;user_data;     switch (req-&gt;type) {     case ACCEPT:         add_accept_request(listen_socket,                           &amp;client_addr, &amp;client_addr_len);         add_read_request(cqe-&gt;res);         io_uring_submit(&amp;ring);         break;     case READ:         if (cqe-&gt;res &lt;= 0) {             add_close_request(req);         } else {             add_write_request(req);         }         io_uring_submit(&amp;ring);         break;     case WRITE:         add_read_request(req-&gt;socket);         io_uring_submit(&amp;ring);         break;     case CLOSE:         free_request(req);         break;     default:         fprintf(stderr, "Unexpected req type %d\n", req-&gt;type);         break;     }     io_uring_cqe_seen(&amp;ring, cqe); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this &lt;code&gt;io_uring&lt;/code&gt; example, the server still requires at least 4 syscalls to process each new client. The only saving achieved here is by submitting a read and a new accept request together. This can be seen in the following strace output for the echo server receiving 1,000 client requests.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;% time     seconds  usecs/call     calls    errors syscall ------ ----------- ----------- --------- --------- ---------------- 99.99    0.445109         111      4001           io_uring_enter   0.01    0.000063          63         1           brk ------ ----------- ----------- --------- --------- ---------------- 100.00    0.445172         111      4002           total&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Combining submissions&lt;/h3&gt; &lt;p&gt;In an echo server, there are limited opportunities for chaining I/O operations since we need to complete a read before we know how many bytes we can write. We could chain accept and read by using a new fixed file feature of &lt;code&gt;io_uring&lt;/code&gt;, but we’re already able to submit a read request and a new accept request together, so there’s maybe not much to be gained there.&lt;/p&gt; &lt;p&gt;We can submit independent operations at the same time so we can combine the submission of a write and the following read. This reduces the syscall count to 3 per client request:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;% time     seconds  usecs/call     calls    errors syscall ------ ----------- ----------- --------- --------- ---------------- 99.93    0.438697         146      3001           io_uring_enter   0.07    0.000325         325         1           brk ------ ----------- ----------- --------- --------- ---------------- 100.00    0.439022         146      3002           total&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Draining the completion queue&lt;/h3&gt; &lt;p&gt;It is possible to combine a lot more work into the same submission if we handle all queued completions before calling &lt;code&gt;io_uring_submit&lt;/code&gt;. We can do this by using a combination of &lt;code&gt;io_uring_wait_cqe&lt;/code&gt; to wait for work, followed by calls to &lt;code&gt;io_uring_peek_cqe&lt;/code&gt; to check whether the completion queue has more entries that can be processed. This avoids spinning in a busy loop when the completion queue is empty while also draining the completion queue as fast as possible.&lt;/p&gt; &lt;p&gt;The pseudocode for the main loop now looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;while (1) {     int submissions = 0;     int ret = io_uring_wait_cqe(&amp;ring, &amp;cqe);     while (1) {         struct request *req = (struct request *) cqe-&gt;user_data;         switch (req-&gt;type) {         case ACCEPT:             add_accept_request(listen_socket,                               &amp;client_addr, &amp;client_addr_len);             add_read_request(cqe-&gt;res);             submissions += 2;             break;         case READ:             if (cqe-&gt;res &lt;= 0) {                 add_close_request(req);                 submissions += 1;             } else {                 add_write_request(req);                 add_read_request(req-&gt;socket);                 submissions += 2;             }             break;         case WRITE:           break;         case CLOSE:             free_request(req);             break;         default:             fprintf(stderr, "Unexpected req type %d\n", req-&gt;type);             break;         }         io_uring_cqe_seen(&amp;ring, cqe);         if (io_uring_sq_space_left(&amp;ring) &lt; MAX_SQE_PER_LOOP) {             break;     // the submission queue is full         }         ret = io_uring_peek_cqe(&amp;ring, &amp;cqe);         if (ret == -EAGAIN) {             break;     // no remaining work in completion queue         }     }     if (submissions &gt; 0) {         io_uring_submit(&amp;ring);     } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result of batching submissions for all available work gives a significant improvement over the previous result, as shown in the following strace output, again for 1,000 client requests:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;% time     seconds  usecs/call     calls    errors syscall ------ ----------- ----------- --------- --------- ---------------- 99.91    0.324226        4104        79           io_uring_enter   0.09    0.000286         286         1           brk ------ ----------- ----------- --------- --------- ---------------- 100.00    0.324512        4056        80           total&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The improvement here is substantial, with more than 12 client requests being handled per syscall, or an average of more than 60 I/O ops per syscall. This ratio improves as the server gets busier, which can be demonstrated by enabling logging in the server:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;% time     seconds  usecs/call     calls    errors syscall ------ ----------- ----------- --------- --------- ---------------- 68.86    0.225228          42      5308       286 write 31.13    0.101831        4427        23           io_uring_enter   0.00    0.000009           9         1           brk ------ ----------- ----------- --------- --------- ---------------- 100.00    0.327068          61      5332       286 total&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows that when the server has more work to do, more &lt;code&gt;io_uring&lt;/code&gt; operations have time to complete so more new work can be submitted in a single syscall. The echo server is responding to 1,000 client echo requests, or completing 5,000 socket I/O operations with just 23 syscalls.&lt;/p&gt; &lt;p&gt;It is worth noting that as the amount of work submitted increases, the time spent in the &lt;code&gt;io_uring_enter&lt;/code&gt; syscall increases, too. There will come a point where it might be necessary to limit the size of submission batches or to enable submission queue polling in the kernel.&lt;/p&gt; &lt;h2&gt;Benefits of network I/O&lt;/h2&gt; &lt;p&gt;The main benefit of &lt;code&gt;io_uring&lt;/code&gt; for network I/O is a modern asynchronous API that is straightforward to use and provides unified semantics for file and network I/O.&lt;/p&gt; &lt;p&gt;A potential performance benefit of &lt;code&gt;io_uring&lt;/code&gt; for network I/O is reducing the number of syscalls. This could provide the biggest benefit for high volumes of small operations where the syscall overhead and number of context switches can be significantly reduced.&lt;/p&gt; &lt;p&gt;It is also possible to avoid cumulatively expensive operations on busy servers by pre-registering resources with the kernel before sending &lt;code&gt;io_uring&lt;/code&gt; requests. File slots and buffers can be registered to avoid the lookup and refcount costs for each I/O operation.&lt;/p&gt; &lt;p&gt;Registered file slots, called fixed files, also make it possible to chain an accept with a read or write, without any round-trip to user space. A submission queue entry (SQE) would specify a fixed file slot to store the return value of accept, which a linked SQE would then reference in an I/O operation.&lt;/p&gt; &lt;h2&gt;Limitations&lt;/h2&gt; &lt;p&gt;In theory, operations can be chained together using the &lt;code&gt;IOSQE_IO_LINK&lt;/code&gt; flag. However, for reads and writes, there is no mechanism to coerce the return value from a read operation into the parameter set for the following write operation. This limits the scope of linked operations to semantic sequencing such as "write then read" or “write then close” and for accept followed by read or write.&lt;/p&gt; &lt;p&gt;Another consideration is that &lt;code&gt;io_uring&lt;/code&gt; is a relatively new Linux kernel feature that is still under active development. There is room for performance improvement, and some &lt;code&gt;io_uring&lt;/code&gt; features might still benefit from optimization work. &lt;/p&gt; &lt;p&gt;&lt;code&gt;io_uring&lt;/code&gt; is currently a Linux-specific API, so integrating it into cross-platform libraries like libuv could present some challenges.&lt;/p&gt; &lt;h2&gt;Latest features&lt;/h2&gt; &lt;p&gt;The most recent features to arrive in &lt;code&gt;io_uring&lt;/code&gt; are multi-shot accept, which is available from 5.19 and multi-shot receive, which arrived in 6.0. Multi-shot accept allows an application to issue a single accept SQE, which will repeatedly post a CQE whenever the kernel receives a new connection request. Multi-shot receive will likewise post a CQE whenever newly received data is available. These features are available in Fedora 37 but are not yet available in RHEL 9.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;io_uring&lt;/code&gt; API is a fully functional asynchronous I/O interface that provides unified semantics for both file and network I/O. It has the potential to provide modest performance benefits to network I/O on its own and greater benefit for mixed file and network I/O application workloads.&lt;/p&gt; &lt;p&gt;Popular asynchronous I/O libraries such as libuv are multi-platform, which makes it more challenging to adopt Linux-specific APIs. When adding &lt;code&gt;io_uring&lt;/code&gt; to a library, both file I/O and network I/O should be added to gain the most from io_uring's async completion model.&lt;/p&gt; &lt;p&gt;Network I/O-related feature development and optimization work in &lt;code&gt;io_uring&lt;/code&gt; will be driven primarily by further adoption in networked applications. Now is the time to integrate &lt;code&gt;io_uring&lt;/code&gt; into your applications and I/O libraries.&lt;/p&gt; &lt;h2&gt;More information&lt;/h2&gt; &lt;p&gt;Explore the following resources to learn more: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/"&gt;Faster IO through io_uring&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://https://kernel.dk/io_uring.pdf"&gt;Detailed description (PDF)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://lwn.net/Articles/863071/"&gt;Fixed files&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kernel.dk/axboe-kr2022.pdf"&gt;What’s new (PDF)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/axboe/liburing/wiki/io_uring-and-networking-in-2023"&gt;io_uring and networking in 2023&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Find other tutorials on Red Hat Developer's &lt;a href="http://developers.redhat.com/topics/linux/"&gt;Linux topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/12/why-you-should-use-iouring-network-io" title="Why you should use io_uring for network I/O"&gt;Why you should use io_uring for network I/O&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Donald Hunter</dc:creator><dc:date>2023-04-12T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus Newsletter #31 - April</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-newsletter-31/" /><author><name>James Cobb</name></author><id>https://quarkus.io/blog/quarkus-newsletter-31/</id><updated>2023-04-12T00:00:00Z</updated><content type="html">It’s time for the April Newsletter. Read Willem Meints’s article "Quarkus: Jave revisited!" where shares his experience with Java using Quarkus after a long hiatus. Learn why Sebastian Daschner thinks Quarkus is a great choice for Enterprise Java in his great video. Learn how to deploy a Quarkus application to...</content><dc:creator>James Cobb</dc:creator></entry><entry><title type="html">How to configure WildFly with YAML files</title><link rel="alternate" href="https://www.mastertheboss.com/jbossas/jboss-configuration/how-to-configure-wildfly-with-yaml-files/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/jbossas/jboss-configuration/how-to-configure-wildfly-with-yaml-files/</id><updated>2023-04-11T12:40:48Z</updated><content type="html">WildFly 28 includes support for YAML configuration which is offers a more flexible approach in some use cases. In this tutorial we will discuss which are the best scenarios where YAML configuration is a perfect fit and how to configure WildFly to use YAML files Why YAML Files? One of the main advantages of YAML ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
